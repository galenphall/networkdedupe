\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{enumitem}

\title{orgnamematch: A Statistical Model for Merging Organizational Nodes in Network Data}
\author{}
\date{\today}

\begin{document}

\maketitle

\section*{orgnamematch}
This repository addresses a specific problem often faced by social scientists studying large-scale organizational network data: how to merge nodes that actually refer to the same entity. Here are some motivating examples:

\begin{enumerate}
\item A \href{https://www.cambridge.org/core/journals/state-politics-and-policy-quarterly/article/chorus-a-new-dataset-of-state-interest-group-policy-positions-in-the-united-states/6827DC9EC72301016894F265777C0078}{large dataset of lobbying records} contains entries with the form ``Org $i$ lobbied for / against Bill $j$.'' This is a bipartite multiplex network. Errors in data entry mean that in some cases, Orgs $i$ and $k$ are actually the same organization. The only way we can recognize these cases is by looking at the plaintext names of the two organizations and their locations in the network.
\item A corporate board interlocks dataset where firms are nodes and edges represent shared directors---for example, ``JP Morgan Chase \& Co.'' vs. ``J.P. Morgan Chase''---requires merging variants of the same company.
\item A nonprofit affiliation network listing charitable organizations under slightly different labels, such as ``World Health Organization'' vs. ``WHO'' or ``World Health Org.'', all referring to the same entity.
\item A political campaign finance dataset recording PAC contributions with names like ``Sierra Club Political Committee'' vs. ``Sierra Club PC,'' which must be consolidated for accurate network analysis.
\end{enumerate}

\section{Model}
We have two types of information to go off of: each node's position in the network, and the text of each node's name. Sometimes two names obviously reference the same organization, as in ``Pfizer'' vs. ``Pfzer, Inc.''. In this case nodes $i$ and $k$ almost certainly ought to be merged regardless of their positions in the network. Other times, two names may look quite similar but in fact reference different organizations, as in ``American Medical Association'' vs. ``American Marketing Association''. In these cases the network position of nodes $i$ and $k$ becomes more important.

We model this problem formally using a generative statistical model. We assume there is a ground truth graph $G = (V, E)$, where $E = \{E_1, \ldots E_l\}$ in the multiplex case. Each node $v_i \in V$ has an associated `name' $n_i$. In practice these names are strings. But for the purpose of our model, we assume each string can equally be represented by an embedding in a $d$-dimensional vector space $D$. Let us call the input space of one-hot encoded names $N$. Then there is a one-to-one and onto mapping $f : N \rightarrow D$ which maps names $n_i$ to embeddings $d_i$.

A ``corruption'' process $C$ acts on $G$ as follows:

\begin{itemize}
\item Each node $v_i \in V$ is split into $k$ nodes $\{v_i^{(1)} \ldots v_i^{(k)}\}$ with probability $p_k = e^{(-\beta k)}$, where $\beta$ is a scale parameter.
\item The edges incident on $v_i$ are allocated randomly among the $v_i^{(j)}$.
\item The `name' of each $v_i^{(j)}$ is corrupted. Formally, a corruption amounts to a random perturbation of the name's embedding $d_i \in D$: $d_i \rightarrow d_i + \eta$, where $\eta \sim \mathcal{N}(0, \sigma)$, i.e. the name perturbation is Gaussian distributed in embedding space.
\end{itemize}

\subsection{Likelihood}
Let 
\begin{itemize}
\item $G = (V, E)$ be the unobserved ``true'' graph with node-names $\{d_i\}_{i \in V}$ in embedding space $D$.
\item $G' = (V', E')$ be the observed corrupted graph with embeddings $\{d'_m\}_{m \in V'}$.
\item $M: V' \to V$ be the unknown many-to-one mapping of corrupted nodes back to originals.
\item $k_i = |M^{-1}(i)|$ be the number of splits of true node $i$.
\end{itemize}

We assume the following factorization of the joint likelihood:
$$P(G', D', M | G, \beta, \sigma) = P(M | \beta) \cdot P(E' | E, M) \cdot P(D' | G, M, \sigma).$$

\begin{enumerate}
\item \textbf{Splitting prior}\\
For each $i \in V$,
$$p(k_i | \beta) = \frac{e^{-\beta k_i}}{Z(\beta)},$$
so
$$P(M | \beta) = \prod_{i \in V} \frac{e^{-\beta k_i}}{Z(\beta)}$$

\item \textbf{Edge-allocation likelihood}\\
Each true edge $(i,j) \in E$ is assigned to exactly one corrupted copy of $i$ and one of $j$, uniformly at random. If $\deg_i$ is the true degree of $i$, then
$$P(E' | E, M) = \prod_{(i,j) \in E} \frac{1}{k_i \cdot k_j}.$$

\item \textbf{Name-corruption likelihood}\\
Given true embedding $d_i$, each corrupted copy $m \in M^{-1}(i)$ has
$$d'_m \sim \mathcal{N}(d_i, \sigma^2 I).$$
Hence
$$P(D' | G, M, \sigma) = \prod_{i \in V} \prod_{m \in M^{-1}(i)} \mathcal{N}(d'_m; d_i, \sigma^2 I).$$
\end{enumerate}

Putting it all together, the joint data likelihood is
$$P(G', D' | G, \beta, \sigma) = \sum_{M} \left[ \prod_{i \in V} \frac{e^{-\beta k_i}}{Z(\beta)} \right] \left[ \prod_{(i,j) \in E} \frac{1}{k_i k_j} \right] \left[ \prod_{i \in V} \prod_{m \in M^{-1}(i)} \mathcal{N}(d'_m; d_i, \sigma^2 I) \right].$$

\subsection{Remarks and extensions}

\begin{itemize}
\item One can place a prior on $G$ (e.g.\ an ERGM) and on the true embeddings $\{d_i\}$.
\item The partition function $Z(\beta)$ may be intractable; one typically uses pseudo-likelihood or variational inference over $M$ and $G$.
\item If names are observed as strings, incorporate a deterministic mapping $f: N \rightarrow D$ before corruption.
\item Inference alternates between estimating $M$ (node merges) and the parameters $(\beta, \sigma, G)$.
\item The marginal likelihood over $\beta, \sigma$ is obtained by integrating or summing out $M$ and $G$.
\item Further model refinements could allow edge-specific noise or non-Gaussian name perturbations.
\end{itemize}

\section{Inference Strategy}
In this section, we outline a complete inference strategy for the orgnamematch model that can scale to networks with thousands of nodes. The strategy employs variational inference with specific optimizations for computational efficiency.

\subsection{Name Embedding}
We begin by creating high-quality name embeddings using a siamese neural network architecture that is fine-tuned on a training set of organization name pairs:

\begin{itemize}
\item The siamese network consists of twin networks with shared weights that map organization names to a $d$-dimensional embedding space.
\item During training, we minimize the Euclidean distance between embeddings of name variants referring to the same organization, while maximizing distance between embeddings of names referring to different organizations.
\item The loss function is defined as a contrastive loss:
$$\mathcal{L}(n_1, n_2, y) = y \cdot \|f(n_1) - f(n_2)\|_2^2 + (1-y) \cdot \max(0, m - \|f(n_1) - f(n_2)\|_2)^2$$
where $y$ is 1 if $n_1$ and $n_2$ refer to the same organization, 0 otherwise, and $m$ is a margin parameter.
\end{itemize}

\subsection{Variational Inference Framework}
We use variational inference to approximate the posterior distribution $P(M|G', D', \beta, \sigma)$ with a tractable distribution $q(M)$:

\begin{itemize}
\item We adopt a mean-field approximation where $q(M) = \prod_{m \in V'} q(M(m))$, assuming independence between node assignments.
\item Each $q(M(m))$ is a multinomial distribution over possible true nodes that the observed node $m$ could map to.
\item Our goal is to minimize the Kullback-Leibler divergence:
$$\text{KL}(q(M) \| P(M|G', D', \beta, \sigma))$$
\item This is equivalent to maximizing the Evidence Lower Bound (ELBO):
$$\mathcal{L}(q) = \mathbb{E}_{q(M)}[\log P(G', D', M | \beta, \sigma)] - \mathbb{E}_{q(M)}[\log q(M)]$$
\end{itemize}

\subsection{Inference Algorithm}
We propose a scalable algorithm that alternates between updating the variational distribution $q(M)$ and the model parameters $\beta$ and $\sigma$:

\begin{algorithm}
\caption{Scalable Inference for orgnamematch}
\begin{algorithmic}[1]
\State \textbf{Input:} Observed graph $G' = (V', E')$, observed name embeddings $\{d'_m\}_{m \in V'}$
\State \textbf{Output:} Estimated mapping $M$, parameters $\beta, \sigma$
\State \textbf{Initialize:} $q(M)$, $\beta$, $\sigma$
\State Apply Locality-Sensitive Hashing (LSH) to embeddings $\{d'_m\}$ to create candidate clusters
\State \textbf{Repeat until convergence:}
\State \quad \textbf{Update $q(M)$:}
\State \quad \quad For each observed node $m \in V'$:
\State \quad \quad \quad Update $q(M(m))$ using coordinate ascent:
\State \quad \quad \quad $q(M(m) = i) \propto \exp\{-\beta \mathbb{E}[k_i] - \sum_{j:(i,j) \in E} \log(k_i k_j) - \frac{\|d'_m - \mathbb{E}[d_i]\|^2}{2\sigma^2}\}$
\State \quad \quad \quad where the expectations are taken over the current $q(M)$
\State \quad \textbf{Prune unlikely mappings:}
\State \quad \quad For each $m \in V'$, keep only the top-$k$ most probable values of $M(m)$
\State \quad \textbf{Update parameters:}
\State \quad \quad $\beta \leftarrow \arg\max_\beta \mathbb{E}_{q(M)}[\log P(M|\beta)]$
\State \quad \quad $\sigma \leftarrow \arg\max_\sigma \mathbb{E}_{q(M)}[\log P(D'|G, M, \sigma)]$
\State \textbf{Extract final mapping:}
\State \quad For each $m \in V'$, set $M(m) = \arg\max_i q(M(m) = i)$
\end{algorithmic}
\end{algorithm}

\subsection{Scaling Techniques}
To handle networks with thousands of nodes, we employ several optimization techniques:

\subsubsection{Locality-Sensitive Hashing}
We use LSH to efficiently find potential matching candidates:

\begin{itemize}
\item Hash name embeddings into buckets such that similar names are likely to be hashed to the same bucket.
\item Specifically, we employ SimHash for embedding vectors:
$$h(d) = \text{sign}(Rd)$$
where $R$ is a random projection matrix.
\item This reduces the search space from $O(|V'|^2)$ to $O(|V'|)$ comparisons.
\end{itemize}

\subsubsection{Block Processing}
We exploit the community structure of networks to partition the inference problem:

\begin{itemize}
\item Apply a community detection algorithm (e.g., Louvain method) to identify densely connected subgraphs.
\item Process each community independently in parallel.
\item Merge results at community boundaries using a second-pass refinement.
\end{itemize}

\subsubsection{Pruning Strategy}
To maintain computational efficiency, we implement aggressive pruning:

\begin{itemize}
\item For each node $m \in V'$, maintain only the top-$k$ most probable assignments according to $q(M(m))$.
\item Dynamically adjust $k$ based on convergence criteria and available computational resources.
\item Use a threshold $\tau$ to completely eliminate low-probability assignments:
$$q(M(m) = i) = 0 \text{ if } q(M(m) = i) < \tau \cdot \max_j q(M(m) = j)$$
\end{itemize}

\subsection{Implementation Considerations}

\subsubsection{Memory Efficiency}
To manage memory constraints with large networks:

\begin{itemize}
\item Store $q(M)$ as a sparse matrix, only maintaining non-zero probability assignments.
\item Use minibatch updates for large communities that cannot fit into memory.
\item Implement disk-based storage for intermediate results when processing very large networks.
\end{itemize}

\subsubsection{Annealing Schedule}
We employ an annealing schedule to improve convergence:

\begin{itemize}
\item Introduce a temperature parameter $T$ that starts high and decreases over iterations:
$$q_T(M(m) = i) \propto q(M(m) = i)^{1/T}$$
\item Initially with high $T$, the distribution is more uniform, allowing exploration.
\item As $T$ decreases, the distribution becomes more concentrated, encouraging exploitation.
\end{itemize}

\subsubsection{Convergence Criteria}
We monitor several metrics to determine convergence:

\begin{itemize}
\item Change in ELBO between consecutive iterations: $|\mathcal{L}_t - \mathcal{L}_{t-1}| < \epsilon_1$
\item Change in parameter values: $|\beta_t - \beta_{t-1}| < \epsilon_2$ and $|\sigma_t - \sigma_{t-1}| < \epsilon_3$
\item Change in mapping assignments: $\sum_{m \in V'} \mathbb{I}[\arg\max_i q_t(M(m)=i) \neq \arg\max_i q_{t-1}(M(m)=i)] < \epsilon_4 \cdot |V'|$
\end{itemize}

\section{Empirical Evaluation}
To validate our approach, we would test on both synthetic and real-world datasets:

\begin{itemize}
\item \textbf{Synthetic validation:} Generate ground-truth networks, apply corruption processes with known parameters, and evaluate recovery accuracy.
\item \textbf{Real-world application:} Apply to actual organizational networks with known duplicates, comparing against manual merging and alternative automated approaches.
\item \textbf{Scalability assessment:} Measure runtime and memory usage as functions of network size and connectivity.
\end{itemize}

\end{document}